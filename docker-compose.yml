version: "3"
services:
  llm-server:
    container_name: llm-server
    hostname: boyou.home.local
    image: ollama/ollama:latest
    ports:
      - "127.0.0.1:11434:11434" 
    volumes:
      - /Users/boyouzho/llm-config:/root/.ollama
    restart: unless-stopped

  web-ui:
    container_name: web-ui
    image: ghcr.io/open-webui/open-webui:main
    volumes:
      - /Users/boyouzho/llm-config/open-webui:/app/backend/data
    ports:
      - "127.0.0.1:3000:8080"
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"
    
